//
//  NetlifyBlogPostsMigration.swift
//  aPrincipalEngineer
//
//  Created by Tyler Thompson on 12/3/24.
//

import Fluent
import Foundation

struct NetlifyBlogPostsMigration: AsyncMigration {
    enum MigrationError: Error {
        case cannotFindTyler
    }
    func prepare(on database: any Database) async throws {
        let prodTyler = try await User.find(UUID(uuidString: "c6b2b5d9-cfcf-4d5d-b818-12d9e4fcd6c3"), on: database)
        let debugTyler = try await User.find(UUID(uuidString: "6D50873A-D6EE-4B1E-9ADF-99067B2B4467"), on: database)
        guard let tyler = prodTyler ?? debugTyler else { throw MigrationError.cannotFindTyler }

        let engineering = Tag(id: UUID(uuidString: "75642bd6-f139-4c4a-b77e-3e93a5dcaab4"), canonicalTitle: "engineering")
        try await engineering.save(on: database)
        let testing = Tag(id: UUID(uuidString: "9a2e3819-bade-418a-abde-15f947452b67"), canonicalTitle: "testing")
        try await testing.save(on: database)
        let swift = Tag(id: UUID(uuidString: "4926c474-487f-4b6f-8c9f-862026a5d005"), canonicalTitle: "swift")
        try await swift.save(on: database)
        let career = Tag(id: UUID(uuidString: "a79c8461-63ed-4663-a11f-b98b02a55e41"), canonicalTitle: "career")
        try await career.save(on: database)
        let visionPro = Tag(id: UUID(uuidString: "83139341-4348-4ff2-bfc3-8fae7e0d271b"), canonicalTitle: "vision-pro")
        try await visionPro.save(on: database)
        let apple = Tag(id: UUID(uuidString: "b462869f-b46e-4ed6-8a47-1dd1b9c289be"), canonicalTitle: "apple")
        try await apple.save(on: database)
        let WWDC = Tag(id: UUID(uuidString: "675afeab-70c5-4e00-b39c-5686b3ed780d"), canonicalTitle: "WWDC")
        try await WWDC.save(on: database)

        let restNetworkingLayer = BlogPost(id: UUID(uuidString: "b35c209e-d2c1-475b-86af-333294c9407e"),
                                           status: .published,
                                           title: "A Great REST Networking Layer",
                                           createdAt: Date(timeIntervalSince1970: 1667151000), // 2022-10-30 11:30
                                           description: "I walk through my favorite REST networking layer, and why I prefer it.",
                                           content: "## Motivation\nA great networking layer is hard to come by. Some people pull in 3rd party dependencies with interceptors and lots of layers of abstraction, and some just use `URLSession` and GCD. I personally don't like either option, at least for simple REST calls. This article will be a bit long, but I'll walk you through my preferred networking layer and explain why I like it.\n\n> NOTE: All code for this example can be found [on my GitHub random projects page](https://github.com/Tyler-Keith-Thompson/RandomSideProjects/tree/main/RESTNetworkLayer)\n\n## GCD, Combine, or Async/Await\nLet's be honest, Grand Central Dispatch (GCD) is kind of a mess. The closure-based APIs aren't very friendly and they're error-prone. Most people who write asynchronous operations using GCD don't even consider cancellation. GCD's quirkiness is why you see network layers with interceptor patterns. This was fine a few years ago, but I think we can do better.\n\n`async/await` is [fraught](https://wojciechkulik.pl/ios/swift-concurrency-things-they-dont-tell-you?utm_campaign=iOS%2BDev%2BWeekly&utm_medium=web&utm_source=iOS%2BDev%2BWeekly%2BIssue%2B582) [with](https://swiftsenpai.com/swift/actor-reentrancy-problem/) [perils](https://alejandromp.com/blog/the-importance-of-cooperative-cancellation/) and people don't often immediately notice them. This is especially true with the cooperative cancellation paradigm, which requires you to be smart about checking whether a task has been cancelled frequently (ideally, after every `await` boundary).\n\nThis is why I prefer Combine, Apple's reactive framework. Its declarative interface, cancellation model, and flexibility with [backpressure](https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7) are incredibly useful when designing a networking layer. I would argue that it is still preferable to `async/await`. Although I would use `async/await` for on-device concurrency concerns.\n\nWhat's more, Combine forces users to store an `AnyCancellable`, and the most common methods of storing them result in appropriate cancellation. For example, if you store a `Set<AnyCanellable>` on a `UIViewController` or SwiftUI `@StateObject`, they are all cancelled when the view is removed from the hierarchy. So, if a user were to hit the \"back\" button in a navigation stack, for example, all ongoing requests for that view would simply cancel.\n\n## Service design\nIdeally, other parts of the code utilize the network layer [through a service](https://en.wikipedia.org/wiki/Service_(systems_architecture)). For example, if I had an API that stored and retrieved posts on a forum, I'd create a `PostService` that returned deserialized `Post` objects. Other parts of my code would ask the `PostService` for things, and it would either reach out over the network, pull from a cache, or any other number of things.\n\nTo that end, our network layer should make it easy for a service to use it with extreme flexibility, but not expose things outside of those services. I think a protocol is a great way of handling this. What if we had something like this:\n\n```\nprotocol PostService {\n    var getPosts: AnyPublisher<Result<[Post], Error>, Never> { get }\n}\n\nstruct _PostService: RESTAPIProtocol, PostService {\n    var baseURL = \"https://api.myforum.com\"\n    var getPosts: AnyPublisher<Result<[Post], Error>, Never> {\n        self.get(endpoint: \"posts\") { request in \n            request\n            .addingBearerAuthorization(accessToken: User.shared.accessToken)\n            .receivingJSON()\n        }\n        .catchHTTPErrors()\n        .catchUnauthorizedAndRetryRequestWithFreshAccessToken()\n        .map(\\.data)\n        .decode(type: [Post].self, decoder: JSONDecoder())\n        .map(Result.success)\n        .catch { Just(.failure($0)) }\n        .eraseToAnyPublisher()\n    }\n}\n```\n\nConsumers only know about `PostService` which exposes a way to get posts, but the service itself (`_PostService`) knows lots of details, like the base URL, endpoint, the fact it needs authentication, it can handle backpressure issues like receiving a 401 and retrying the request, it knows we're using REST and JSON and it knows how to deserialize into an array of `Post`. \n\n## Creating a `RESTAPIProtocol`\nOne quirk of making requests with Swift is that you get a `URLResponse` which needs to be converted into an `HTTPURLResponse` to check things like the status code. To make this easier, our protocol should return an `HTTPURLResponse`.\n\nLet's start with a simple protocol definition:\n```\nimport Foundation\nimport Combine\n\npublic protocol RESTAPIProtocol {\n    typealias ErasedHTTPDataTaskPublisher = AnyPublisher<(data: Data, response: HTTPURLResponse), Error>\n    typealias Output = ErasedHTTPDataTaskPublisher.Output\n    typealias Failure = ErasedHTTPDataTaskPublisher.Failure\n\n    var baseURL: String { get }\n    var urlSession: URLSession { get }\n}\n\n@available(iOS 13.0, macOS 11.0, tvOS 13.0, watchOS 7.0, *)\nextension RESTAPIProtocol {\n    public var urlSession: URLSession { URLSession.shared }\n\n    public func get(endpoint: String) -> ErasedHTTPDataTaskPublisher {\n        guard let url = URL(string: \"\\(baseURL)\")?.appendingPathComponent(endpoint) else {\n            return Fail<Output, Failure>(error: URLError(.badURL)).eraseToAnyPublisher()\n        }\n        var request = URLRequest(url: url)\n        request.httpMethod = \"GET\"\n        return createPublisher(for: request)\n    }\n\n    // Other verbs, put/post/patch/delete\n\n    func createPublisher(for request: URLRequest) -> ErasedHTTPDataTaskPublisher {\n        Just(request)\n            .flatMap { [urlSession] in\n                urlSession.dataTaskPublisher(for: $0)\n            }\n            .tryMap {\n                guard let res = $0.response as? HTTPURLResponse else {\n                    throw URLError(.badServerResponse)\n                }\n                return (data: $0.data, response: res)\n            }\n            .eraseToAnyPublisher()\n    }\n}\n```\n\nOur protocol now exposes a way to make `GET` requests... but it doesn't allow people to modify the outgoing request. Consumers of our protocol want 2 specific behaviors:\n\n- The ability to modify a request before it is sent.\n- If the publisher retries (like when a 401 is returned) then the request modifier should be recalculated.\n    - To expand on this idea, look at the example in our `PostService` when the publisher chain restarts the *new* access token needs to be used, not the old one.\n\nBecause `Just` is a little fiddly, anything we put in there will be cached, we need to be a little bit clever. Let's modify `RESTAPIProtocol`\n\n```\npublic protocol RESTAPIProtocol {\n    typealias RequestModifier = ((URLRequest) -> URLRequest)\n    ...\n}\n\n@available(iOS 13.0, macOS 11.0, tvOS 13.0, watchOS 7.0, *)\nextension RESTAPIProtocol {\n    ...\n\n    public func get(endpoint: String, requestModifier: @escaping RequestModifier = { $0 }) -> ErasedHTTPDataTaskPublisher {\n        ...\n        return createPublisher(for: request, requestModifier: requestModifier)\n    }\n\n    func createPublisher(for request: URLRequest, requestModifier: @escaping RequestModifier) -> ErasedHTTPDataTaskPublisher {\n        Just(request)\n            .flatMap { [urlSession] in\n                urlSession.dataTaskPublisher(for: requestModifier($0))\n            }\n            ...\n    }\n}\n```\n\nNow consumers can modify a request just like in our `PostService` example. Calculating the `requestModifier` in the `flatMap` gives us the behavior we want when the chain is restarted.\n\n## Fluent request modification\nYou may have noticed that in my proposed service, we used a [fluent API](https://en.wikipedia.org/wiki/Fluent_interface#Swift). This not only fits well with Combine, which is already fluent, but it makes it easy to compose sets of headers. Here's how we can do that:\n\n```\nextension URLRequest {\n    public func addingValue(_ value: String, forHTTPHeaderField header: String) -> URLRequest {\n        var request = self\n        request.setValue(value, forHTTPHeaderField: header)\n        return request\n    }\n}\n```\n\nThis also has the advantage of not mutating the original request, avoiding mutation where we can is generally of great benefit. You'll notice the existing `URLRequest` is copied into a new variable, then *that* is modified using Apple APIs.\n\n## Error handling\nWe can create a series of `HTTPError` types, some for 400-499 `HTTPClientError` types and some for 500-599 `HTTPServerError` types. These can even peak into a request and find standard headers that give more error info. For example, a 429 usually comes with a `Retry-After` header indicating how long you should wait before attempting the request again.\n\nOnce those error types are created, we can create a Combine modifier that handles them, here's an example:\n\n```\nextension Publisher {\n    public func catchHTTPErrors() -> Publishers.TryMap<Self, Output> where Output == RESTAPIProtocol.Output {\n        tryMap {\n            guard let err: any HTTPError = HTTPClientError(code: UInt($0.response.statusCode)) ?? HTTPServerError(code: UInt($0.response.statusCode)) else {\n                return $0\n            }\n\n            if $0.response.statusCode == 429 {\n                throw HTTPClientError.tooManyRequests(retryAfter: $0.response.retryAfter)\n            }\n\n            throw err\n        }\n    }\n}\n```\n\nUsers may also want to be able to catch specific kinds of errors, which Combine doesn't quite allow on its own. This gives them the ability to add custom logic to the request chain. Here's an example that responds to rate limiting (a 429)\n\n```\nextension Publisher {\n    public func tryCatch<E: Error & Equatable,\n                         P: Publisher>(_ error: E,\n                                       _ handler: @escaping (E) throws -> P) -> Publishers.TryCatch<Self, P> where Failure == Error {\n        tryCatch { err in\n            guard let unwrappedError = (err as? E),\n                    unwrappedError == error else { throw err }\n            return try handler(unwrappedError)\n        }\n    }\n\n    public func respondToRateLimiting(maxSecondsToWait: Double = 1) -> AnyPublisher<Output, Failure> where Output == RESTAPIProtocol.Output, Failure == Error {\n        catchHTTPErrors()\n            .tryCatch(HTTPClientError.tooManyRequests()) { err -> AnyPublisher<Output, Failure> in\n                guard case .tooManyRequests(let retryAfter) = err else {\n                    throw err // shouldn't ever really happen\n                }\n\n                let delayInSeconds: Double = {\n                    if let serverDelay = retryAfter?.converted(to: .seconds).value,\n                       serverDelay < maxSecondsToWait {\n                        return serverDelay\n                    }\n                    return maxSecondsToWait\n                }()\n\n                return Just(()).delay(for: .seconds(delayInSeconds),\n                                      scheduler: DispatchQueue.global(qos:.userInitiated),\n                                      options: nil)\n                .flatMap { _ in self.catchHTTPErrors() }\n                .eraseToAnyPublisher()\n            }\n            .eraseToAnyPublisher()\n    }\n}\n```\n\nThere are a few complicated Combine-type things to learn, but look at just how easy it is to handle rate limiting! No interceptors and complex retry logic, just a simple combination of existing Combine operators. I'll leave it as an exercise for the reader to imagine how you could add even more flexibility (like retrying on a 401) to this. Alternatively, check out [the GitHub repo](https://github.com/Tyler-Keith-Thompson/RandomSideProjects/tree/main/RESTNetworkLayer) to see an example.\n\n## Testing\nOkay, so while reactive programming might be new to people, this whole layer isn't too intimidating. But how hard is it to test? I personally use [OHTTPStubs](https://github.com/AliSoftware/OHHTTPStubs) and create my own [fluent wrapper around it](https://github.com/AliSoftware/OHHTTPStubs/issues/349) to make this dead simple.\n\nLet's start by defining a Combine test helper:\n```\nextension Publisher {\n    func firstValue(timeout: TimeInterval = 0.3,\n                    file: StaticString = #file,\n                    line: UInt = #line) async -> Result<Output, Error> where Failure == Error {\n        await withCheckedContinuation { continuation in\n            var result: Result<Output, Error>?\n            let expectation = XCTestExpectation(description: \"Awaiting publisher\")\n\n            let cancellable = map(Result<Output, Error>.success)\n                .catch { Just(.failure($0)) }\n                .sink {\n                    result = $0\n                    expectation.fulfill()\n                }\n\n            XCTWaiter().wait(for: [expectation], timeout: timeout)\n            cancellable.cancel()\n\n            do {\n                let unwrappedResult = try XCTUnwrap(\n                    result,\n                    \"Awaited publisher did not produce any output\",\n                    file: file,\n                    line: line\n                )\n                continuation.resume(returning: unwrappedResult)\n            } catch {\n                continuation.resume(returning: .failure(error))\n            }\n        }\n    }\n}\n```\n\nNow I'll show you how easy it is to test our rate-limiting logic:\n```\nimport Foundation\nimport Combine\nimport XCTest\n\nimport OHHTTPStubs\nimport OHHTTPStubsSwift\n\nimport RESTNetworkLayer\n\nfinal class HTTPOperatorsTests: XCTestCase {\n    struct JSONPlaceholder: RESTAPIProtocol {\n        var baseURL: String = \"https://jsonplaceholder.typicode.com\"\n    }\n\n    override func setUpWithError() throws {\n        HTTPStubs.removeAllStubs()\n\n        stub { _ in true } response: { req in\n            XCTFail(\"Unexpected request made: \\(req)\")\n            return HTTPStubsResponse(error: URLError.init(.badURL))\n        }\n    }\n\n    func testCatchingTooManyRequests() async throws {\n        let url = try XCTUnwrap(URL(string: \"https://www.google.com\"))\n\n        let error = HTTPClientError.tooManyRequests()\n\n        let response = try XCTUnwrap(HTTPURLResponse(url: url,\n                                                     statusCode: Int(error.statusCode),\n                                                     httpVersion: nil,\n                                                     headerFields: [\"Retry-After\": \"1.5\"]))\n\n        let result = await Just((data: Data(), response: response))\n            .setFailureType(to: Error.self)\n            .catchHTTPErrors()\n            .firstValue()\n\n        guard case .failure(let failure) = result else {\n            XCTFail(\"Publisher succeeded, expected failure with HTTPError\")\n            return\n        }\n\n        guard let actualError = failure as? (any HTTPError) else {\n            XCTFail(\"Error: \\(failure) thrown by publisher was not an HTTPError\")\n            return\n        }\n\n        XCTAssertEqual(actualError.statusCode, error.statusCode)\n\n        if case HTTPClientError.tooManyRequests(.some(let retryAfter)) = actualError {\n            XCTAssertEqual(retryAfter.converted(to: .seconds).value, 1.5)\n        } else {\n            XCTFail(\"RetryAfter value not in error.\")\n        }\n    }\n\n    func testRetryAfterServerSpecifiedTime() async throws {\n        let json = try XCTUnwrap(\"\"\"\n        [\n            {\n                userId: 1,\n                id: 1,\n                title: \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n                body: \"quia et suscipit suscipit recusandae consequuntur expedita et cum reprehenderit molestiae ut ut quas totam nostrum rerum est autem sunt rem eveniet architecto\"\n            },\n        ]\n        \"\"\".data(using: .utf8))\n        let retryAfter = Double.random(in: 0.100...0.240)\n        let requestDate: Date = Date()\n        StubResponse(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            HTTPStubsResponse(data: Data(), statusCode: Int32(HTTPClientError.tooManyRequests().statusCode), headers: [\"Retry-After\": \"\\(retryAfter)\"])\n        }\n        .thenRespond(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            HTTPStubsResponse(data: json, statusCode: 200, headers: nil)\n        }\n\n        let api = JSONPlaceholder()\n\n        let value = try await api.get(endpoint: \"posts\")\n            .respondToRateLimiting()\n            .firstValue()\n            .get()\n\n        XCTAssertGreaterThan(Date().timeIntervalSince1970 - requestDate.timeIntervalSince1970, Measurement(value: retryAfter, unit: UnitDuration.milliseconds).converted(to: .seconds).value)\n        XCTAssertEqual(value.response.statusCode, 200)\n        XCTAssertEqual(String(data: value.data, encoding: .utf8), String(data: json, encoding: .utf8))\n    }\n\n    func testRespondToRateLimitingOnlyRetriesOnce() async throws {\n        let retryAfter = Double.random(in: 0.100...0.300)\n        let requestDate: Date = Date()\n        StubResponse(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            HTTPStubsResponse(data: Data(), statusCode: Int32(HTTPClientError.tooManyRequests().statusCode), headers: [\"Retry-After\": \"\\(retryAfter)\"])\n        }\n        .thenRespond(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            HTTPStubsResponse(data: Data(), statusCode: Int32(HTTPClientError.tooManyRequests().statusCode), headers: [\"Retry-After\": \"\\(retryAfter)\"])\n        }\n        .thenRespond(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            XCTFail(\"Should not have made a 3rd request\")\n            return HTTPStubsResponse(data: Data(), statusCode: Int32(HTTPClientError.tooManyRequests().statusCode), headers: [\"Retry-After\": \"\\(retryAfter)\"])\n        }\n\n        let api = JSONPlaceholder()\n\n        var publisherRetries = 0\n        let result = await api.get(endpoint: \"posts\")\n            .map { val in\n                publisherRetries += 1\n                return val\n            }\n            .respondToRateLimiting(maxSecondsToWait: 0)\n            .firstValue()\n\n        XCTAssertGreaterThan(Date().timeIntervalSince1970 - requestDate.timeIntervalSince1970, Measurement(value: retryAfter, unit: UnitDuration.milliseconds).converted(to: .seconds).value)\n        XCTAssertThrowsError(try result.get()) { error in\n            guard let actualError = error as? (any HTTPError) else {\n                XCTFail(\"Error: \\(error) thrown by publisher was not an HTTPError\")\n                return\n            }\n\n            XCTAssertEqual(actualError.statusCode, HTTPClientError.tooManyRequests().statusCode)\n        }\n        XCTAssertEqual(publisherRetries, 2)\n    }\n\n    func testRateLimitingShouldDoNothingUnlessCorrectStatusCodeIsGiven() async throws {\n        let json = try XCTUnwrap(\"\"\"\n        [\n            {\n                userId: 1,\n                id: 1,\n                title: \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n                body: \"quia et suscipit suscipit recusandae consequuntur expedita et cum reprehenderit molestiae ut ut quas totam nostrum rerum est autem sunt rem eveniet architecto\"\n            },\n        ]\n        \"\"\".data(using: .utf8))\n        StubResponse(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            HTTPStubsResponse(data: json, statusCode: 200, headers: nil)\n        }\n\n        let api = JSONPlaceholder()\n\n        let value = try await api.get(endpoint: \"posts\")\n            .respondToRateLimiting()\n            .firstValue()\n            .get()\n\n        XCTAssertEqual(value.response.statusCode, 200)\n        XCTAssertEqual(String(data: value.data, encoding: .utf8), String(data: json, encoding: .utf8))\n    }\n\n    func testRateLimitingDoesNotRetryIfADifferentErrorIsThrown() async throws {\n        var requestCount = 0\n        StubResponse(on: isAbsoluteURLString(\"https://jsonplaceholder.typicode.com/posts\") && isMethodGET()) { _ in\n            requestCount += 1\n            return HTTPStubsResponse(data: Data(), statusCode: 401, headers: nil)\n        }\n\n        let api = JSONPlaceholder()\n\n        let result = await api.get(endpoint: \"posts\")\n            .respondToRateLimiting()\n            .firstValue()\n\n        XCTAssertThrowsError(try result.get()) {\n            XCTAssertEqual($0 as? HTTPClientError, .unauthorized)\n        }\n\n        XCTAssertEqual(requestCount, 1)\n    }\n}\n```\n\nThere may be a lot of code, but each test is actually quite simple and understandable.\n\n## Wrapping up\nIt's fair to say this probably isn't a beginner-level networking layer. But the power and flexibility of Combine, coupled with the cancellation model make it a really useful tool. This article certainly didn't cover all the details, check out [the git repo](https://github.com/Tyler-Keith-Thompson/RandomSideProjects/tree/main/RESTNetworkLayer) to see even more of how it all came together.",
                                           author: tyler)
        try await restNetworkingLayer.save(on: database)
        try await restNetworkingLayer.$tags.attach([engineering, testing, swift], on: database)
        
        let automatedTestingUndesirableChange = BlogPost(id: UUID(uuidString: "1665046f-18ba-4005-9122-b708a28640dc"),
                                                         status: .published,
                                                         title: "Automated Testing: Ability to Catch Undesirable Change",
                                                         createdAt: Date(timeIntervalSince1970: 1658338200), // 2022-07-20 11:30
                                                         description: "The ability to catch undesirable change is the crux of automated testing, let's dive into the details of how we should create tests that catch undesirable change.",
                                                         content: "## The undesirable change principle\n**Ability to catch undesirable change**: This is one of the primary purposes of a test harness. Undesirable change usually means bugs. Automated tests report on change in a codebase. Make sure reported changes are undesirable. \n\n## Coverage is not a good primary metric\nIn teams where you use measurements to encourage good practices it can be common for people to set team goals to have high coverage numbers. What is coverage? Many test runners can measure which parts of production code are exercised by tests. When all tests are finished it can show how much production code is \"covered\", and which parts are not covered. Coverage is a useful metric but it doesn't tell you that tests will catch undesirable change, it only tells you that tests exercise production code.\n\nIn [the last article](https://www.aprincipalengineer.com/blog/automated-testing-false-positive-rate/index.html) we talked about mutation testing and a mutation score. If you combine a mutation score with coverage this can be a *better* metric. It'll tell you that not only is something covered, but that it catches logical errors.\n\n## Write your tests early in your development cycle\nThis is a great time to talk about TDD (Test Driven Development) as a practice. TDD involves writing a test before you write any production code. This practice has a host of benefits, for one you get naturally high test coverage, this method also encourages architectures that are testable. If we lived in a perfect world, it'd be worthwhile using TDD to develop everything.\n\nTDD isn't always possible, it's a skill that takes a long time to master and developers may not have that skill or have the desire to hone it. Even for those who use TDD regularly there are times when there are too many unknowns to write a test first. In these cases, there's a different solution.\n\n## Ensure tests fail, before they pass\nWhether you use TDD or not it's crucial to make sure you've witnessed a test failing for the right reason, ideally before you see it passing for the right reason. If your test is written first this is as simple as running it. If your production code is written first, comment out that production code, then run your test, it should fail. Uncomment the production code and run your test again, it should pass.\n\nBy witnessing tests fail you gain confidence they assert desired behavior. Tests that assert on desirable behavior will catch when that desirable behavior isn't working. In other words, those tests will likely catch undesirable change.\n\n## Write lots of tests\nEach test you write should target some small piece of behavior. Make sure you keep writing tests until those tests observe all behavior that's important to you. By continuing to write tests until they observe all important behavior you'll catch undesirable changes better. Remember that automated tests aren't there to make your code bug-free, but they should drastically reduce the severity/likelihood of bugs making it to production.\n\n## Name your tests wisely\nWhen tests fail the name of the failing test is shown, as well as what assertions failed. These test names should give you a very good idea of what has gone wrong. `testFoo` doesn't help anybody, but `testWhenEnteringAValidUsernameAndPassword_TheUserIsLoggedIn` describes very adequately what the test should be proving.\n\n## Run your tests regularly\nWe [focused so heavily on cost](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html) first for this moment. Tests catch changes when they're run, you should make sure they're run frequently so that you get feedback early. It's also important to run tests as a natural, and largely unavoidable part of your process. CI/CD pipelines are perfect for this, every time somebody commits to your codebase you can run your tests. Because commits represent changes, this is a naturally perfect time to run your tests.\n\nTests running on a cadence can sometimes be valuable but beware of this, if you've got a separate pipeline that runs tests every week it's much easier to ignore than tests that run on every change. This has to do with where pain is felt. Running tests on commits means that you can prevent those commits from being merged and part of production code. However, when running on a cadence it's unlikely you'll have the same processes in place to prevent those changes from going live.\n\nThe times when I've run tests on a cadence are times when I'm writing tests for systems I don't control. For example, integration tests on an external service. It's difficult to design these tests so that they give valuable feedback. Sometimes an external service fails the test for the wrong reasons, the more this happens, the more likely it is the suite will be ignored.\n\nIn [the next article](https://www.aprincipalengineer.com/blog/automated-testing-ability-to-support-desirable-change/index.html) we'll cover supporting desirable change.",
                                                         author: tyler)
        try await automatedTestingUndesirableChange.save(on: database)
        try await automatedTestingUndesirableChange.$tags.attach([engineering, testing], on: database)
        
        let automatedTestingDesirableChange = BlogPost(id: UUID(uuidString: "de43ae83-2feb-4b2f-9a7d-25f6078fb547"),
                                                       status: .published,
                                                       title: "Automated Testing: Ability to Support Desirable Change",
                                                       createdAt: Date(timeIntervalSince1970: 1658431800), // 2022-07-21 1:30
                                                       description: "Supporting desirable change allows code to be refactored with ease. It involves writing tests that aren't coupled to implementation details and it can make working in a well-tested codebase a much nicer experience.",
                                                       content: "## The desirable change principle\n**Ability to support desirable change**: A good automated testing suite isn't coupled to implementation details, and is instead focused on desired behavior. This means that if you want to change to a new architecture or completely refactor your codebase, your test harness supports that change.\n\n## From interaction to expected result\nIn [a previous article](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html) I talked about Sociable unit testing. This style of testing will serve us well for the conversation on supporting desirable change. An example of desirable change would be a refactor, it's the same observable behavior, but perhaps with code that's more readable, or less code.\n\nThe kinds of tests that support desirable change the most tend to be tests that act like a consumer, and assert on end results. For example, in a mobile app that means interacting with the UI like a user, and asserting that some observable behavior happens. This doesn't necessarily mean using a UI testing framework and it definitely doesn't mean starting every test from app launch. It can mean calling the function that triggers on button press, or it could mean using a view testing framework to tap a button.\n\nWhen it comes to expected results, keep these observable. I had a conversation recently about a unit test we were writing to ensure a list of times was sorted. My colleague suggested that we could just inspect an array and assert the array was sorted. I suggested that we should inspect the view and assert the displayed views were sorted. If we tested the array we were relying on an implementation detail and something as simple as renaming the array breaks the test. There's also a false positive potential because there was no test proving the array is what backed the view.\n\n## Structure of tests\nLet's break down \"Arrange, Act, Assert\" or \"Given, When, Then.\" These are both methods of structuring your tests. They refer to the same concept, you start a test by setting up preconditions. For example, logging in a user, or stubbing out a network layer. You then perform actions, for example tapping a login button. Finally, you assert. To continue the example, you'd assert that a user is logged in, and likely assert that they landed on a home screen.\n\nUsing these methods can be a great way to know where to stop one test and start another. In general, you should keep tests to one group of \"Arrange, Act, Assert.\" There will always be exceptions but it's a good rule of thumb.\n\nIt's okay for tests to not be DRY. For those unfamiliar DRY stands for \"Don't Repeat Yourself\" and is a good practice when writing production code. It encourages sharing similar code. However, when writing tests this can cause problems. I might say to have your tests be damp, don't share setup based on data, or configuration. If your tests share too much setup code, they might prove brittle whenever preconditions for one test change.\n\nYou should also strive to design your tests to be completely independent. If one test depends on another happening first, you're creating a test suite that will be hard to maintain. If you really want to drive this home most test runners allow for randomized execution order. \n\n## Don't test configuration\nWe build configuration into systems because it might change frequently. For example, you might configure URLs to use for network calls, colors, fonts, and localized strings. Testing this configuration usually doesn't provide valuable feedback and the effort is normally quite large.\n\nInstead, test where configuration is *consumed* and assert that it's consumed correctly. For example, in the case of localized strings you can supply your own test value and assert it's displayed when a view renders. This gives the flexibility that was desired with configuration but confidence that the configuration will be used correctly.\n\n## Don't test design details\nMuch like configuration design details are rarely worth testing. For example, testing padding on a view is generally a waste of a test. The padding could change and the test would report a failure, but that failure doesn't necessarily give valuable feedback. Designs change frequently, and colors, padding, styling, fonts, and other details are more about delight than functionality. \n\nDo test important functionality as it relates to design. For example, do test that details from a user's profile are displayed. It shouldn't matter to your automated tests *where* they are displayed, just that the information is on the screen somewhere.\n\nIn [the next article](https://www.aprincipalengineer.com/blog/automated-testing-proximity-to-code/index.html) we'll go over keeping automated tests close to production code.",
                                                       author: tyler)
        try await automatedTestingDesirableChange.save(on: database)
        try await automatedTestingDesirableChange.$tags.attach([engineering, testing], on: database)
        
        let automatedTestingCost = BlogPost(id: UUID(uuidString: "fa44f008-9f2e-4152-af22-c9359d1d206c"),
                                            status: .published,
                                            title: "Automated Testing: Cost",
                                            createdAt: Date(timeIntervalSince1970: 1658172600), // 2022-07-18 1:30
                                            description: "Learn how to minimize cost when writing automated tests. Learn about the different kinds of tests you might write and where to invest your time.",
                                            content: "## The cost principle\n**Cost**: This is often measured in time. If two tests harnesses prove the same thing, but one executes in a few seconds and the other in a few minutes, prefer the one that executes faster. Fast tests give developers immediate feedback and are more likely to be run repeatedly.\n\n## Disambiguating testing terminology\nThe way we refer to automated tests is very ambiguous. Let's start by identifying some terminology and strategies for writing tests. \n\n#### Unit tests\nOne of my favorite questions to ask somebody who writes automated tests is \"What is a unit?\" This is a puzzler for so many people, so let me clearly describe it here. A unit is a single interface, now some languages have a nominal type called `Interface` where you define an abstraction layer...that's not what I mean. Every structure in code has an interface of some kind, it has an internal interface and sometimes a public interface. For example:\n\n```swift\npublic struct MyStruct {\n    let property = \"someProperty\"\n\n    func someFunc() { }\n\n    public func somePublicFunc() { }\n\n    private func somePrivateFunc() { }\n}\n```\n\nThe preceding example has an internal and a public interface. The internal interface has a property called `property` and a function called `someFunc`. These are a unit. It also has a public interface that has `somePublicFunc`, that is also a unit. This didn't use an `Interface` (or in Swift, a `Protocol`) but it still has an interface. Other parts of the codebase will call the internal functions and external consumers will call the public method. A series of unit tests should be written to cover each interface. \n\nThe private method in this example is not part of the unit and it should not be tested. Testing private method tightly couples your tests to implementation details, it doesn't matter what a private method does or how many private methods there are. What matters is when you use an interface there's a desired result. When a consumer calls `someFunc` they have an expectation of behavior. Our tests should assert what that expectation is.\n\n#### Sociable tests\nMartin Fowler wrote an excellent article on [Sociable Testing](https://martinfowler.com/bliki/UnitTest.html). In it he coins the terms *Solitary* and *Sociable* when referring to unit tests. A solitary unit test is one that isolates all collaborators. In other words, if one structure (class/struct/etc) depends on another, you create a test double for that dependency. Sociable tests assume that other collaborators work, and do not isolate the unit. \n\nI'll take a strong stance here and suggest that this is where you should invest your time and effort. Sociable unit tests allow for great refactoring without worrying about implementation details. There are still times when you need to inject a test double, for example if your structure makes a remote network call you might want to stub network responses. However, if you assume other dependencies work you can still pinpoint issues but more importantly refactors don't cause tests to break for the wrong reasons.\n\nSociable tests are still unit tests because you're still testing a unit, you want to ensure that some interface, when invoked, behaves according to expectations. That said, when you write sociable tests it may not be necessary to write them for every unit of production code. If we take this to the extreme you could write tests that perform actions as if a User had, and then assert on the expected behavior. Allowing whatever architecture you desire to exist and be refactored.\n\n[ViewInspector](https://github.com/nalexn/ViewInspector) is a library I use for testing SwiftUI. It is SwiftUI unit testing, they're fast and effective, but they perform actions much like a user would. It can do hit testing and check on view state, like whether a button is enabled. You can write tests that set up preconditions, perform actions like a user would, then assert on expected behavior. Any implementation details about what structures exist in the production code are irrelevant. [UIUTest](https://github.com/nallick/UIUTest) is a library that does the same, but for UIKit.\n\n#### Mocks, stubs, and spies.\nA spy is a test double that reports back on interactions on a unit. So if you had a function `doSomething` a spy would report how many times `doSomething` was called and with what parameters. A stub is a structure that returns canned responses. If `doSomething` returns a `String` you could write a stub that said when `doSomething` is called, return some test value. These are great for creating a test double of a dependency. A Mock is much like a stub, except like a spy, it can verify interactions. For example, a mock can stub a response and verify whether `doSomething` was called and with which parameters.\n\nSpies can often be a code smell, they can indicate that you're relying on implementation details. Stubs and mocks can be incredibly powerful tools for ensuring your test suite doesn't rely on external dependencies. For example, you'll frequently stub your network layer when writing unit tests. You'll also mock things like a database, because you don't want your unit tests creating any persistent state.\n\nMocks, stubs, and spies should all be used in conjunction to remove side-effects from testing. Your tests should be atomic, in other words every time you run them they should produce the same results. If unit tests are writing files, or entries to a database, or making network calls then your tests will likely not be atomic, the second time your run them they may produce different results. This is not at all desirable.\n\n#### Contract tests\nUnit tests are the cheapest kind of tests, they're amazingly fast and very reliable. If you can prove everything you need to with unit tests, you should not write other kinds of tests. That said, often times a system you're writing will communicate with others. In these cases you are relying on those external systems to respond in a particular way. Contract tests are a simple way of determining whether an external system is adhering to the agreed upon contract. [Pact.io](https://pact.io) is a wonderful example of a contract testing framework that works asynchronously. Their tagline changed in recent years and they claim to be an integration testing framework, but I think they're still far more suited to contract testing. There are other services, like [Wiremock](https://wiremock.org/) that are also useful for contract testing. \n\nUsing a service like pact is great because it validates both systems are adhering to the contract. Wiremock could be useful, but might also be achievable with a simple HTTP stubbing library. Either way your system makes assumptions about a contract, these tests document those assumptions and assert that they're correct.\n\n#### Integration tests\nIntegration tests are more expensive that contract tests. If you can prove all you need to with unit and contract tests, you can avoid writing integration tests. That said, sometimes verifying a contract is met isn't enough. For example, if your system depends not only on the contract being correct but also observable behavior being correct in a different system. An example might be creating a resource, then deleting that resource behaves as expected. \n\nWhile there are integration testing tools a lot of them start violating the principal of proximity to your code. For example you can maintain postman scripts for integration testing, but those aren't as close to your production code as another test target that performs network calls would be. I recommend writing integration tests simply, in the same language you wrote your production code in.\n\nIf it's at all possible create ephemeral versions of your external dependencies. In other words, if you're writing a mobile app's integration tests you would be better off starting a docker container with an API your app depends on and testing against that over the local network than actually hitting a production or pre-production hosted version of that API. This puts your tests in closer proximity to your code and it reduces a lot of noise for issues like network latency and bandwidth constraints. \n\n#### End-to-end tests\nEnd to end tests are the most expensive kind of tests you can write. These should be incredibly limited. End-to-end tests should be thought of more as a sanity check than anything else. They should cover only your critical features and shouldn't test edge cases, but instead should test what most users of your system will do. In order to execute an End-to-end (e2e) test you have to stand up every part of your environment and test through it. \n\nOnce again, you're better off if you can stand up your entire environment in an ephemeral way. For example, using something like docker compose to stand up a version of every microservice in an environment and a database. After the test is over these can all be destroyed. This isolates you as best as possible from noisy and irrelevant conditions, like a different team working on a service and it not being available at that moment.\n\nIf your e2e testing suite is the only one that catches a legitimate problem this should be a major red flag. The expectation is that unit, contract, and integration tests will have found any issues LONG before an e2e suite executes. These tests are often very slow and very prone to false negatives because of timeouts.\n\n## Reducing cost\nThink of every kind of test you might write in terms of expense. If they take a long time to execute and are brittle they are more expensive. If they execute very quickly and consistently then they are cheap. As much as you possibly can, push towards the cheapest tests you can write. I've released very large projects that are backed entirely by sociable unit tests and I had extreme confidence in them. I've also seen teams bogged down with hours of end-to-end tests that failed half the time. It's not a great place to be in. \n\nThe cheaper your tests are, the more you can run them. Your e2e suite might take 20 minutes and only be run when you're about to release, but your unit testing suite will likely be run constantly while developing and as part of CI pipeline. Given that the whole point is fast feedback for developers you want to be executing your tests as frequently as possible.\n\n[The next article](https://www.aprincipalengineer.com/blog/automated-testing-false-negative-rate/index.html) will cover false negative rates and how to lower them.",
                                            author: tyler)
        try await automatedTestingCost.save(on: database)
        try await automatedTestingCost.$tags.attach([engineering, testing], on: database)
        
        let automatedTestingFalseNegative = BlogPost(id: UUID(uuidString: "4d4b70cc-1bbd-4505-a40e-33376d494302"),
                                                     status: .published,
                                                     title: "Automated Testing: False Negative Rate",
                                                     createdAt: Date(timeIntervalSince1970: 1658252700), // 2022-07-19 11:45
                                                     description: "Learn how to minimize false negative rates writing automated tests. Learn about the different processes and tools that can help with a low false negative rates.",
                                                     content: "## The false negative principle\n**False negative rate**: Tests that fail for the wrong reasons are disastrous. The more this happens the more likely teams are to ignore their tests. An ignored test suite is not providing value. Therefore, a test harness with a lower false negative rate is preferable.\n\n## What causes false negatives?\nFalse negatives are often caused by sources external to your system. For example, an integration test might fail because of network latency, an e2e or slow UI test might fail because of an animation. There's also issues with shared testing environments. If one test tries to read a resource and another tries to delete it, you might end up with with a race condition that causes a false negative.\n\nLastly, the wrong assertion can easily create false negatives. One advantage to writing tests before you write production code to satisfy those tests, is that your assertions tend to be better. If you find yourself writing assertions based on implementation details you could be creating conditions for a false negative after a refactor. Assertions should be all about consumer expectations. If you have a button on a screen that triggers a network request, and on completion shows a confirmation screen, arrange your test by stubbing the network response, then tap the button, then assert the correct network request was sent and that the confirmation view was shown. Those assertions are about user expectations, they are the most important details about this hypothetical flow.\n\n## How to stop false negatives?\nStart by eliminating external dependencies as much as you reasonably can. Did you need an integration test or can it be a contract test? Do you need to actually send a request of HTTP or can you stub your network layer? Then eliminate causes of delays. For example, disable animations when running your tests. Finally, avoid sleeping for a set duration like the plague. If your test must wait, wait on a specific condition, like a network call completing, or a view rendering, or a control becoming enabled. It's okay to wait with a timeout, but adding `sleep` to your automated test is a surefire way to cause false negatives and brittle tests.\n\nI also suggest avoiding prototypical UI testing frameworks. Appium, XCUITest, selenium, and others like them are incredibly slow frameworks. While they can provide value in an e2e suite, you're better off using a faster and more reliable framework. For example, using [ViewInspector](https://github.com/nalexn/ViewInspector) to test SwiftUI, or [UIUTest](https://github.com/nallick/UIUTest) to test UIKit. Both of these are unit testing frameworks, but they give a lot of the same value that UI testing frameworks give. For example, they perform hit testing, they can throw an error if you try and tap on a disabled button, and more. They're also much more consistent and reliable.\n\nIn [the next article](https://www.aprincipalengineer.com/blog/automated-testing-false-positive-rate/index.html) we'll go over details on false positive rates and how to lower them.",
                                                     author: tyler)
        try await automatedTestingFalseNegative.save(on: database)
        try await automatedTestingFalseNegative.$tags.attach([engineering, testing], on: database)
        
        let automatedTestingFalsePositive = BlogPost(id: UUID(uuidString: "fde72d71-6b59-49a6-bca5-419c12835348"),
                                                     status: .published,
                                                     title: "Automated Testing: False Positive Rate",
                                                     createdAt: Date(timeIntervalSince1970: 1658345400), // 2022-07-20 1:30
                                                     description: "Learn how to minimize false positive rates writing automated tests. Learn about the different processes and tools that can help with a low false positive rates.",
                                                     content: "## The false positive principle\n**False positive rate**: Tests with a high false positive rate don't cause immediate pain, they cause pain later on. This undermines confidence in all tests and can ruin efforts to get automated tests in place. Therefore, a test harness with a lower false positive rate is preferable.\n\n## Feel pain as soon as possible\nIf pain is going to be felt, it is best that pain is felt early. Reflect on this statement because it applies to virtually every aspect of software development and beyond. In a lot of ways this is why we even write tests to begin with. If there's some kind of undesirable behavior we want to know about it right away so that we can fix it. This also touches on concepts like crash-first development, which we'll go over is some later post.\n\nOne of the reasons we focus on cheap tests is so that we can frequently run them. Ideally all tests are run with every change, if your test harness is lightweight this is totally achievable and gives extreme confidence. This means after making a change locally you can run tests, it also means your CI pipeline can and should run tests. It should also deliberately fail if tests do not pass. These are ideal setups and it's worth putting a lot of effort into creating.\n\n## What causes false positives?\nFalse positives always have the same cause, some part of your test is incorrect. It could be that when you setup preconditions for your tests, those preconditions weren't valid. It could be that when your test acts it's not acting like a user really would, and thus isn't giving the right feedback. The most frequent I see is that assertions are the wrong assertions. Your assertions might be asserting on something irrelevant or they might not actually be happening at all.\n\nLet's talk about those assertions. When your test asserts something, you want to make sure it's asserting expected behavior. When I see tests written by those who are unfamiliar with automated testing I almost always see some flavor of `assertNotNull(thingThatCannotBeNullAnyways)`. I'll also sometimes see a test set up an expectation but then fulfill that expectation without exercising production code. These will increase your coverage numbers, but are a waste of computing resources.\n\n## How can we detect false positives?\nIt's a little expensive in terms of compute power but [Mutation Testing](https://en.wikipedia.org/wiki/Mutation_testing) is a great way of detecting false positives. The short version is that mutation testing will create \"mutants\" in your production code, it might change `>` to `<` or `==` to `!=`. It'll then run your tests, the code should either refuse to compile or the tests should fail. If the production code is mutated and tests do not fail, that mutant will be reported and it'll hurt your mutation score. Ideally, you want your tests to catch all mutants as they represent undesirable behavior.\n\nI love mutation testing as a sanity check but because it's having to recompile code frequently it takes a very long time to run. Because mutation tests are a sanity check I don't recommend running them as frequently. I've set them up to on a cadence for projects under active development. These tests will go a long ways towards identifying false positives and giving you more confidence in your test suite.\n\nIn [the next article](https://www.aprincipalengineer.com/blog/automated-testing-ability-to-catch-undesirable-change/index.html) we'll talk about catching undesirable change, mutation testing can be a very valuable tool to check that your tests catch undesirable change.",
                                                     author: tyler)
        try await automatedTestingFalsePositive.save(on: database)
        try await automatedTestingFalsePositive.$tags.attach([engineering, testing], on: database)
        
        let automatedTestingProximity = BlogPost(id: UUID(uuidString: "733037c5-960e-4ed8-8190-d1fd72fb24d6"),
                                                 status: .published,
                                                 title: "Automated Testing: Proximity to Production Code",
                                                 createdAt: Date(timeIntervalSince1970: 1658462400), // 2022-07-21 22:00
                                                 description: "Tests that are closer to production code are easier to execute. Let's dive into details about how to keep your tests close to your production code.",
                                                 content: "## The proximity to code principle\n**Proximity to code**: Given the choice between 2 test harnesses that prove the same thing, one executing locally and one executing in the cloud, use the local test suite. Having tests live in close proximity to code encourages tests to be frequently run, it can mean that your tests can execute without internet access, and it means that they're convenient to sanity check commits and other small units of work in real time.\n\n## Keep tests in the same repo as production code\nI've often seen teams choose to move tests that are slow or brittle to a different repository with a different pipeline. This is inevitably a bad solution because the tests will certainly be ignored. Instead, refer to [the cost principle](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html) and move those tests to a suite that isn't as slow or brittle.\n\nIn many ways it'd be better to outright delete tests than it would be to move them to a different repo. If tests are moved to a different repository sometimes tests that are valuable will be added there, but it makes it so inaccessible during standard development those good tests won't be run. It's a similar concept to deleting a flaky feature rather than hiding it behind a feature flag. You don't want to clutter things up just because you invested in it once.\n\n## Tests should be painful when they fail\nRemember we focused on [false negative rate](https://www.aprincipalengineer.com/blog/automated-testing-false-negative-rate/index.html) as a metric. If tests have a high false negative rate it'd be better to delete them than it is to skip them or change your pipeline so that test failures don't stop it.\n\nTest failures should be painful, because test failures should be giving you valuable feedback that something doesn't work. When I see teams change their processes so that test failures don't stop progress I see those teams miss important information. I've had many times when I've seen a team comment out a test, or skip it just to find out later that they broke something and that test was pointing them to the issue.\n\nKeep your test failures painful, make them stop releases, prevent merges, and fail pipelines. Focus on false negative rates and remember that it's better to delete a test than it is to ignore it.\n\n## Beware cloud-hosted test harnesses\nThere are so many services out there that like to boast making testing easy. Many of them use cloud hosting and cloud-based test running. This takes your tests away from your production code and it makes them require an internet connection to run. Neither of those situations is ideal. \n\nRemember, our tests must have the [ability to catch undesirable change](https://www.aprincipalengineer.com/blog/automated-testing-ability-to-catch-undesirable-change/index.html). That involves frequently running them, ideally every time there is a change. It'll be much harder to run them frequently if they exist in a completely different environment to your production code. \n\n## Be mindful of testing culture\nI am very wary of separate testing teams or QA engineers that don't pair with software engineers. Many of the problems discussed in this article crop up when you've got separate testing and implementation teams. In my entire career I've quite literally never seen this setup work well. Cultural problems inevitably creep up and the test suite suffers for it.\n\nOne consequence is a \"not my problem\" kind of mentality that springs up. If developers that implement a feature are not responsible for testing that feature they also don't take responsibility when tests fail. Additionally, the people that do write automated tests want their own controlled environment and extract tests to a different repository, causing all problems to get worse.\n\nEven with very mature teams I've seen cultural breakdown. For example, people will choose not to write unit tests because somebody else is going to write UI tests. This violates [the cost principle](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html) and causes the cultural rift and \"not my problem\" mentality to get worse over time.\n\nTo be clear, a QA engineer position can still be valuable, I'm not knocking the role. I'm simply stating that those who write tests need to be very close to those that implement systems. That could mean pairing, or it could mean the same person does all the work. \n\nAs a team member I encourage you to push for unit tests on merge requests, communicate with people who write UI tests and tell them what is already covered, and be the person who doesn't consider code production ready until tests are in place. I think this is a good hill to die on, because untested systems are so much harder to work in. A mature automated testing suite helps everybody, even if it's more work up front.",
                                                 author: tyler)
        try await automatedTestingProximity.save(on: database)
        try await automatedTestingProximity.$tags.attach([engineering, testing], on: database)
        
        let intellectualProperty = BlogPost(id: UUID(uuidString: "bc561082-a34b-4a53-8877-dec62d28ffd8"),
                                            status: .published,
                                            title: "I care about Intellectual Property and so should you!",
                                            createdAt: Date(timeIntervalSince1970: 1658462400), // 2024-01-19 08:00
                                            description: "Learn about intellectual property and how it relates to day-to-day software development.",
                                            content: "## Disclaimer\nThis is the \"I am not a lawyer\" disclaimer. If you're the type of person who finds a relatively obscure personal blog online with musings from an engineer about intellectual property and decides to take it as concrete legal advice you should most definitely stop reading...you should also carefully rethink your strategy there cause it's a good way to get in trouble. \n\nWhat I can say is I've spoken to a quite a few people who are lawyers about intellectual property and software development. My opinions about that have been formed from those conversations as well as my time contributing to open source.\n\n## Why does it matter?\nOkay so there's the obvious answer to this question, you don't want to get sued. There's a different answer though, it matters because of basic ethics and morality. Stealing somebody's ideas or even their unique expression of something more commonplace is really just not something you should do! I recognize how easy it is to forget this when, as an engineer, you're just trying to make something work. However, it's important to take a step back every once in a while and think about more than solving a problem or making a feature function.\n\n## Licenses\nLet's get the basics out of the way, when you use software somebody else wrote that software generally has a license of some kind. The license dictates how you are allowed to use their software and if you do not like the terms of the license your choice is to convince the authors the change it, grant you a different license, or not use the software. Here's the thing, the terms of the license *generally* apply when you distribute your software. This allows you to play with software locally or in a branch without worrying about a license, but still comply with its term when you share it with others. Consequently, however, this means you must be careful about understanding the terms of a license before you distribute your software. When I've spoken to developers about this I've sometimes heard them say, \"But I don't know how to read these licenses.\" This argument is ludicrous to me, if you're capable of understanding code you're capable of reading a license and getting a general idea of what it's asking for. YES sometimes they contain difficult to parse legal language, but getting a general understanding of the requirements doesn't take a lot of know-how. For most common open source licenses there are plenty of resources to help you understand exactly what the requirements are. In fact, GitHub released a feature that broke them down pretty well! If you are engaging with a vendor or working for a company with a legal department it's as easy as asking the legal team your company pays.\n\nI'll lastly point out that when you as a developer choose to include software, whether that's copied from a website, taken as source from a repository, or pulled in with a package manager, you are acting on behalf of your employer in regards to accepting the terms of that license. This responsibility shouldn't be lost on you as you evaluate that cool open source package that makes your life easier. It's your job as a developer not only to understand whether the technical trade-off is worthwhile but also if the license is acceptable.\n\n## Copyleft Licenses\nSo one potential \"gotcha\" when you're just trying to write code is [copyleft licenses](https://en.wikipedia.org/wiki/Copyleft). There are multiple examples of this, the [GPL](https://en.wikipedia.org/wiki/GNU_General_Public_License) is one that you're likely to run into on GitHub. I love the premise of copyleft licenses. What if we quit acting like software was some closely guarded secret and instead made it open? What if everybody could see exactly what code was executing on their hardware? This utpoic vision of open software is something I firmly believe in! Copyleft licenses (or \"viral\" licenses) try to force that vision into a reality. When you use a copyleft license you agree that any works you use it in will ALSO use that license. So if you pull in GPL licensed software then you agree that whatever you distribute will ALSO be GPL licensed. Most companies don't like this because then it forces them to open source their products, which they are generally loathed to do. Realistically, I wish companies were less stingy about this because oftentimes what people pay for are services, not apps. Having an open-source app wouldn't necessarily ruin somebody's business. However, most companies are totally unwilling to accept those terms because they want a closed-source system.\n\n## Attribution\nMost licenses for things you'll run across online have an attribution requirement. Back to my disclaimer up top please, please, go talk to a real lawyer your company employs to understand what they believe satisfies the attribution requirement of licenses. HOWEVER in my experience and opinion you have not satisfied attribution requriements unless end users are able to see the attribution. So NO, it generally doesn't count if you just have a comment in your code that mentions the license or has the license text. Now you may try to get clever here and point out that code comments could potentially be seen by users with interpreted languages and a debug console as long as the code wasn't minified...that's true but it's also sort of a ridiculous argument. Look, for open source software especially somebody put their personal time and effort to give you something you can use that's useful, the least you could do is let your users know that they built a cool thing!\n\n## The Stack Overflow Problem\nI get it, Stack Overflow is a great website! When you're just trying to solve a problem it routinely has an answer from somebody else who WANTS to help you solve your problem. However, even though this isn't code from a package manager it *still* has a license. What most people are unaware of is that stack overflow answers are [licensed under a copyleft creative commons license](https://stackoverflow.com/help/licensing). This is particularly alarming because the creative commons license stack overflow uses isn't code compatible. Even if you have open source software you're building that is GPL licensed, GPL isn't compatible with the creative commons license. The CC license is really more to protect literary works or media such as art. Code, because it is executable, has a different set of licenses that typically apply. To be abundantly clear if you just have a code comment that links to a stack overflow answer you have in no way satisfied the license or attribution requirements! All you've done is provided proof that you stole code and stuck it in your codebase.\n\n## Intellectual Property Policies\nThe other aspect to all this is what a company might impose on you. I was trapped by the same thing that so many others get trapped by. When I was a young, early-career software engineer getting my first job in the private sector and I had 1,000 documents to sign for my employment contract I just signed them all happily. I trusted the company to create fair policies and that generally worked out until I started wanting to contribute to open source. So almost every company has the same policy \"we own anything you develop while you work for us that is related to our business or you created on hardware we provided you.\" This seems almost reasonable, certainly your employer should own the intellectual property that you create as part of your job, that's just fair. The ownership over things you wrote using their hardware may be a little overbroad in certain circumstances, but it's acceptable, if you want to develop something not work related you can just go do that on a personal device. Here's where it can all fall apart, what about intellectual property that's related to their business? Well, to some extent this makes sense. If you work for a financial company they teach you a lot about the finance world and they don't want you creating competing projects using industry knowledge they helped you gain. Where this becomes unfair to employees is if you independently acquired knowledge in finance and wanted to build something, now you can't without your company owning it. It's nearly impossible to codify and enforce those differences or prove them in the event of litigation or a lawsuit so generally you just have to accept this. Now, what happens if you work for a consulting company that works across a variety of industries? Or what happens if you work for a gigantic Apple/Google/Microsoft type company that does EVERYTHING? Well, it means exactly what you think it means, any software you write becomes the property of your employer. This is decidedly unfair and you should really spend a lot of time understanding your IP policy for your company. If you do not like the policy DO NOT SIGN IT! This could mean you don't take a job, or it could mean you get a change added to it.\n\n## Non-Compete Policies\nYou know both IP policies and non-compete policies can really kill innovation or the desire to innovate at a company. If your employer insist on hoarding intellectual property and doesn't allow you to work in the same field for multiple years after you leave this can really complicate your ability to innovate or even move on. Once more if you work for a gigantic company that does everything with a non-compete you may find you just have to be unemployed or go work in fast food for the entire non-compete duration if you ever leave. I wish companies spent a little more time thinking about how this impacted employees. There have been research studies that directly link these types of policies to a loss of innovation and employee satisfaction. Most employers don't really buy that, but employees who care about these policies are often employees who really want to innovate. For what it's worth this was one of the contributing factors to me leaving a 5 year employer with an overbro…",
                                            author: tyler)
        try await intellectualProperty.save(on: database)
        try await intellectualProperty.$tags.attach([engineering], on: database)
        
        let makingThisBlog = BlogPost(id: UUID(uuidString: "7bc345f7-1587-4fdd-885c-ad254890f67e"),
                                      status: .published,
                                      title: "Making this blog",
                                      createdAt: Date(timeIntervalSince1970: 1650853800), // 2022-04-24 20:30
                                      description: "Learn about how \"A Principal Engineer\" was created using Swift!",
                                      content: "## Motivation\nNow that I've been a Principal Engineer for a few years, I think it's time to try and share some details about how I got where I am. My hope is that others trying to become technical leaders can benefit from some of my lessons learned along the way. So I decided to make a blog. It seems to me that a \"making of\" post is as good a place to start as any.\n\n## Creating the blog code\nMy favorite programming language is Swift. Many people believe it's just for iOS development, but that is untrue. Swift is a fantastic multi-paradigm language that you can use to build virtually anything. If I had to describe myself as an engineer, I would probably say I'm a full-stack Swift developer.\n\nJohn Sundell wrote [Publish](https://github.com/johnsundell/publish), which is an excellent static site generator for Swift. I previously used Publish to write my resume, so it was a natural choice when making the blog. It makes excellent use of Swift's generic system and result builders to create HTML. \n\nI'm not a designer, so I needed a template. I found a wonderful free-to-use template called \"Sparrow 1.0\" and was well on my way. I have got to give the designers huge credit because it's one of the best templates I have ever used. The HTML came formatted, and the CSS was actually understandable. It was even written in such a way that it would not conflict with other CSS you already have. I did notice that it was only a \"light mode\" type template. I added several CSS media queries to support dark mode based on system settings and contributed that code back to the creators of \"Sparrow 1.0\".\n\n## Publishing the site\nPublishing platforms were a little tricky. I wanted something free, or at least very cheap. I explored using GitHub pages and carefully read the terms. While I didn't see anything that indicated I couldn't use GitHub pages, it felt like a blog would go against the spirit of the feature. Instead, I found [Netlify](https://netlify.com/), which is perfect for hosting static websites. \n\n## CI/CD pipeline\nI am a big believer in automation. Netlify does offer an option to just upload a folder and host it, but I simply cannot pass up the opportunity to push to the `main` branch and have my website deployed a few minutes later. Because Publish is pure Swift, it can run on Linux, which means I could use a swift docker container to generate my static site. I used GitHub actions as a CI/CD provider and a [Netlify deploy plugin](https://github.com/jsmrcaga/action-netlify-deploy) to publish the site.\n\n## Pagination\nThe last big hurdle was getting pagination working reasonably well with a static website. I took a look at some implementations of pagination in other static website generators, like Jekyll. Ultimately, I wrote a build plugin for Publish that did the trick. Here's a taste of the code that backs it.\n\n```swift\nextension APrincipalEngineer {\n    static let BLOG_PAGE_SIZE = 10\n}\n\nextension Plugin where Site == APrincipalEngineer {\n    static var generatePaginatedBlogPages: Self {\n        Plugin(name: \"Generated Paginated Blog Pages\") { context in\n            guard let blogSection = context.sections.first(where: { $0.id.rawValue == APrincipalEngineer.SectionID.blog.rawValue }) else {\n                throw PublishingError(\n                    infoMessage: \"Unable to find blog section\"\n                )\n            }\n            let allItems = context.sections.flatMap { $0.items }\n            let pages = allItems.chunks(ofCount: Site.BLOG_PAGE_SIZE).enumerated().map { (offset, _) -> Page in\n                let index = offset + 1\n                let blog = Blog(context: context, section: blogSection, pageSize: Site.BLOG_PAGE_SIZE, offset: Site.BLOG_PAGE_SIZE * offset)\n                return Page(path: Path(\"pages/\\(index)\"), content: .init(title: \"Blog - Page \\(index)\", description: \"A Principal Engineer Blog - Page \\(index)\", body: .init(html: blog.html.render()), date: Date(), lastModified: Date(), imagePath: nil, audio: nil, video: nil))\n            }\n\n            pages.forEach {\n                context.addPage($0)\n            }\n        }\n    }\n}\n```",
                                      author: tyler)
        try await makingThisBlog.save(on: database)
        try await makingThisBlog.$tags.attach([engineering, swift], on: database)
        
        let passionAndPurpose = BlogPost(id: UUID(uuidString: "b51b5885-d3f5-4082-b634-383cd90443ab"),
                                         status: .published,
                                         title: "What motivates employees?",
                                         createdAt: Date(timeIntervalSince1970: 1650907800), // 2022-04-25 11:30
                                         description: "Explore your motivations and how they can affect your career trajectory.",
                                         content: "## What motivates employees?\nI read an article recently that suggested the two motivators for employees are passion and purpose. They broke down passion as \"getting good at a skill\" and purpose as \"contributing to society.\" I think that's an oversimplification, but still worth exploring. \n\nLet's start by defining more clear terms. Let's say that \"getting good at a skill\" can be better defined as \"mastery.\" Let's say that \"contributing to society\" can be better defined as \"impact.\" It's also important to acknowledge that motivations are more complicated than these two concepts. Furthermore, they aren't mutually exclusive. I know that both things are very important to me. \n\n## Mastery\nI often hear product managers, agile coaches, recruiters, and other non-developer types saying that developers value \"interesting\" problems to solve. For myself and many of the people I've worked with over the years, I think this is a mischaracterization. Instead, I might say that developers who care about mastery want to master their preferred stack. You can give me a very interesting problem that's in a technology stack I hate (JavaScript, for example), and I will have a hard time staying engaged for a long period of time.\n\nWhat's more, this is the area where developers will spend time outside work hours, whether that's reading blogs, working on side projects, or going to conferences. If that mastery lines up with a business model, then there's a mutually beneficial arrangement, and employees will be motivated; If that mastery does not line up with a business model, that's okay, as long as employees have the time and space to learn and grow.\n\n## Impact\n\"Impact\" has been important to me personally since before I was a professional software engineer. There's no simple definition here -- sometimes people can measure impact by the number of users reached, sometimes by tangible good done. What I can confidently say is that working on some piece of software that nobody uses is demoralizing. \n\nI regularly see a mistake when communicating with a software development team. People who aren't familiar with code will sit down and tell the team *what* to do. This grossly limits potential and autonomy and leans heavily towards micromanaging. Moreover, it means you're no longer paying smart employees to do what they do well. Instead, focus on *why* they should build something above all else. When a team understands why the thing they're building is important, they can innovate, they can measure, and they can succeed without constant attention.\n\n## Beware exploitation\nI believe that many managers don't intentionally exploit their employees. However, intentional or not, if you are a very passionate and motivated individual, you should take steps to avoid being exploited. This has been a hard lesson for me personally. At a previous job, I had a 216% utilization rate, meaning that I logged enough hours not only to cover my expense but at least one, arguably two other team members' expenses. \n\nFind a way to strike a balance between growing your mastery, making an impact, and pacing yourself. Companies do not adequately reward your extreme efforts. Instead, you'll find that as soon as you start producing more, they expect more. This is a natural consequence of human behavior. What's more, as you start producing enough to cover other team members' you may become resentful that you do not get paid much more. \n\nTo compare to my own example, when I kept up 200% output for over a year, I got a $2,000 bonus. My efforts earned my company around $15 million over three years and probably about $8 million that year. A $2,000 bonus is a joke at that point. It's true that I got a promotion eventually and got a slightly bigger paycheck, but for all that effort, I didn't have much impact on anything except making the CEO more wealthy.\n\n## What's the best motivation?\nThere's no clear answer here. A combination of both mastery and impact becomes important for motivating employees. I will say that the best engineers I have ever had the pleasure of working with cared about making an impact. When those engineers became demotivated, they often said the same thing, \"I learned a lot, but I haven't made any real difference.\"\n\nI think to be a great engineer, you really do have to care about both. I've met people who could write great code but did not care about making an impact. I would not describe those people as \"great\" engineers. Some of them didn't even qualify as adequate. They mostly just stood in the way. I've also met well-meaning people who never focused on mastery. They often did things the hard way and got badly exploited by companies.",
                                         author: tyler)
        try await passionAndPurpose.save(on: database)
        try await passionAndPurpose.$tags.attach([engineering, career], on: database)
        
        let automatedTesting = BlogPost(id: UUID(uuidString: "798d115c-4268-4236-84e9-d290f47cf450"),
                                        status: .published,
                                        title: "Some Thoughts on Automated Testing",
                                        createdAt: Date(timeIntervalSince1970: 1658093400), // 2022-07-17 15:30
                                        description: "My thoughts and opinions on the value of automated testing.",
                                        content: "## Motivation\nI've been practicing TDD (Test Driven Development) for many years now. I don't consider code to be production ready unless it's got some kind of test backing it up. Tests make their way into my side projects, utilities, and even this blog. This article series will delve into details about what, in my opinion, constitutes a good test harness and where you should invest time in automated testing.\n\n## Why write tests?\nTests make their way into everything I do because they give me confidence. With a robust automated testing harness I can release on demand, I can refactor without fear of breaking critical functionality, and I can greatly increase my productivity as projects age. An added benefit is that my testing suite provides documentation for expected behavior of whatever I'm building. I can go back years later and understand exactly what I was trying to do.\n\n## When are tests not valuable?\nAutomated testing is great for repeatable processes, but really bad at judgement. You'll always want human eyes to gauge whether your user experience is good. Automated tests also primarily catch change, this means if something changes frequently tests can become noisy and unhelpful. To that end, I don't test things like colors, fonts, padding, etc..because those things can change very frequently. My tests are entirely focused on expected behavior, things that won't change frequently.\n\n## How do you measure the value of tests?\nFirst, to be clear, even with a mature and robust automated testing suite bugs will get out into production. You know your test harness is useful if the severity/likelihood of bugs found in production is favorable. In other words, an automated testing suite may not catch a critical severity bug that has a very low chance of happening. Or it might not test a very minor but prolific bug. That's acceptable, but a good automated test harness will make sure that the things you care about continue to work. Here's how I measure the value of an automated testing suite.\n\n#### You can measure your automated tests by:\n- **[Cost](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html)**: This is often measured in time. If two tests harnesses prove the same thing, but one executes in a few seconds and the other in a few minutes, prefer the one that executes faster. Fast tests give developers immediate feedback and are more likely to be run repeatedly.\n- **[False negative rate](https://www.aprincipalengineer.com/blog/automated-testing-false-negative-rate/index.html)**: Tests that fail for the wrong reasons are disastrous. The more this happens the more likely teams are to ignore their tests. An ignored test suite is not providing value. Therefore, a test harness with a lower false negative rate is preferable.\n- **[False positive rate](https://www.aprincipalengineer.com/blog/automated-testing-false-positive-rate/index.html)**: Tests with a high false positive rate don't cause immediate pain, they cause pain later on. This undermines confidence in all tests and can ruin efforts to get automated tests in place. Therefore, a test harness with a lower false positive rate is preferable.\n- **[Ability to catch undesirable change](https://www.aprincipalengineer.com/blog/automated-testing-ability-to-catch-undesirable-change/index.html)**: This is one of the primary purposes of a test harness. Undesirable change usually means bugs. Automated tests report on change in a codebase. Make sure reported changes are undesirable. \n- **[Ability to support desirable change](https://www.aprincipalengineer.com/blog/automated-testing-ability-to-support-desirable-change/index.html)**: A good automated testing suite isn't coupled to implementation details, and is instead focused on desired behavior. This means that if you want to change to a new architecture or completely refactor your codebase, your test harness supports that change.\n- **[Proximity to code](https://www.aprincipalengineer.com/blog/automated-testing-proximity-to-code/index.html)**: Given the choice between 2 test harnesses that prove the same thing, one executing locally and one executing in the cloud, use the local test suite. Having tests live in close proximity to code encourages tests to be frequently run, it can mean that your tests can execute without internet access, and it means that they're convenient to sanity check commits and other small units of work in real time.\n\nIn [the next article](https://www.aprincipalengineer.com/blog/automated-testing-cost/index.html) we'll dive into each of these concepts one-by-one and talk about how to maximize and measure each one. For now, be that voice on your team to push for automated testing, don't let tests become an afterthought. You'll find that writing tests encourages better architectural decisions, allows for better release processes, and despite an initial investment, greatly speeds up development.",
                                        author: tyler)
        try await automatedTesting.save(on: database)
        try await automatedTesting.$tags.attach([engineering, testing], on: database)
        
        let visionProPost = BlogPost(id: UUID(uuidString: "5675d0e5-f69d-4b89-a642-6d6ac979ce83"),
                                 status: .published,
                                 title: "Using the Vision Pro with an Irregular Anatomy",
                                 createdAt: Date(timeIntervalSince1970: 1706958000), // 2024-02-03 04:00
                                 description: "Apple's new Vision Pro is a very cool piece of tech, but how does it work for people who don't have a typical anatomy?",
                                 content: "### What's the problem?\nFor those who aren't aware I have an exceedingly rare [condition known as Aniridia](https://www.ncbi.nlm.nih.gov/books/NBK538133/#:~:text=Introduction-,Aniridia%20is%20defined%20as%20a%20partial%20or%20complete%20absence%20of,of%20congenital%20aniridia%20are%20sporadic.). Aniridia is either a partial or complete absence of an Iris. In my case, I have no Iris's at all! This makes the very premise of the Vision Pro rather difficult. For one, it's \"Optic ID\" is an Iris scan, for another tracking somebody's eye movement is almost certainly heavily impacted by somebody's Iris. This is effectively confirmed if you go through the setup process for the Vision Pro which keeps increasing the background lighting and asking you to select various dots by looking at them. My suspicion is that the lighting change is specifically calibrate eye tracking with various eye dilations. In my case, however my eyes are always 100% dilated.\n\n### How did it work?\nSetup for me was an absolute nightmare. It started out easy enough, put on the Vision Pro, become impressed by \"Hello\" being written in the physical world, and enjoy my partner laughing at how stupid I looked waving at thin air with these large awkward goggles strapped to my face. \n\nIt starts off simple enough, bring your phone close and through typical Apple magic it starts to pair with your new device, done! Then an immediate challenge. It asked me to look at the code that came with my prescription lenses. I stared at the code and....nothing. I figure maybe it's just not calibrated and needs help so I set down the code on my table and bent over so it was basically the only thing in my field of view...success!\n\nNext up comes the calibration, hold out my hands...success, flip them over...success. And here's where the Vision Pro experience fell apart for me. Stare at 6 dots and select each one by tapping your fingers together. The Vision Pro couldn't remotely tell where I was looking, random dots would activate nowhere near where I was focused.\n\nAfter making me select 6 dots, failing to know which ones I selected, and making me try again it'd say \"Completing Eye Setup ... Eye Setup Failed.\" Of course there's no information to be found online and contacting Apple support is rather burdensome since they're overwhelmed with calls from folks asking for help. \n\n### So is it Remotely Usable?\nActually, yes! While I couldn't find a reasonable way during the setup to change how tracking worked I was able to triple tap the crown to turn on VoiceOver. For those not familiar VoiceOver is a screen reader designed to help blind people navigate Apple products. This allowed me to get past the setup screens but came with the downside that it kept trying to move the VoiceOver cursor based on where I was looking.\n\nGetting past the setup was still *very* cumbersome because of the VO cursor moving with where it thought I was looking, but eventually I was able to use the virtual keyboard to create a password and get through to the home screen!\n\nA dive into the Accessibility Settings finally gave me what I really wanted, a way to tell the Vision Pro that I don't want it to use eye tracking to move the cursor. There were several options:\n\n- Eyes -- bad for me\n- Head -- This actually works quite well and it's when I went with. The downside is that the Vision Pro is heavy and this involves a fair bit of neck movement\n- Wrist -- I might give this more of a shot, you have a lot more \"waving your hands around\", but it comes with the advantage that when you tap your fingers to select something it's picked up much better and more frequently by the Vision Pro\n- Index Finger -- I liked this the least of the options that worked, it involved a little too much hand movement for my taste\n\nIt's also worth noting there was a way to turn on a display that showed you where the cursor was. This was honestly hilarious when I had \"eyes\" selected because I could see exactly how hard it was for the Vision Pro to track my eye movement, the cursor manically jumped all around my virtual environment.\n\n### What about Optic ID?\nOptic ID, unsurprisingly, doesn't remotely work with my anatomy. It scans my eyes, tells me to raise the headset a bit, then just fails. I honestly never expected this to work, as it was always described as an Iris scan which notably requires somebody to have Iris's. \n\n### Verdict\nApple gets a gold star for having different pointer options, it makes the headset decidedly usable even for people like me. If you've got any number of conditions that would make eye tracking difficult rest assured there is *some* way to use the $3,500+ device. Apple gets a big fat thumbs down for the setup experience being so difficult to navigate when you have an irregular anatomy.",
                                 author: tyler)
        try await visionProPost.save(on: database)
        try await visionProPost.$tags.attach([visionPro, apple], on: database)

        let wwdc2022 = BlogPost(id: UUID(uuidString: "274feaf9-5219-4d3d-9c09-6e8620e52db2"),
                                 status: .published,
                                 title: "WWDC 2022 Overview",
                                 createdAt: Date(timeIntervalSince1970: 1654538400), // 2022-06-06 12:00
                                 description: "My impressions on WWDC 2022, the things I found excited, and areas I intend to explore next.",
                                 content: "## WWDC 2022\nAnother year, another WWDC. It was honestly great to be able to check out the Keynote today. I had a few people texting me for my opinion and about 6 slack threads open with snarky comments and excited chatter. Now I've got a warning on the whole blog but just to rehash this is entirely based on my opinion. Yours may differ, feel free to leave comments!\n\n### The Sexy\nThere's so much in this category this year! I honestly went in uncertain about how much innovation we'd see, especially after how disruptive last year was. I was pleasantly surprised, let's dive in.\n\n- **Live Text**: Did you see this?! I am amazed at how well live text appears to work in iOS 16. This is leaps and bounds for accessibility, for translation, for convenience, for really useful in-app experiences.\n- **Zillow Callout**: Okay, I'm a little bit biased (read: very biased) because I work for Zillow, but how cool was it that we got name dropped by Apple this year?\n- **SwiftUI Enhancements**: You could be forgiven for missing this because it FLASHED past but NavigationView is finally getting an upgrade. It looks like they're exposing control over the navigation stack. This also deserves honorable mention in \"It's about time.\" I am really looking forward to the next few days where I can learn more.\n- **PassKey**: This is great. I want to be clear, Apple doesn't deserve all the credit for WebAuthn, it's a cross-industry effort. That said, the future is here! I absolutely love password-less authentication. This is going to really help mitigate a lot of existing risk. It'll also expose a bunch of new attack vectors, remember, bad actors will always find new ways of getting data.\n- **WeatherKit**: I recognize there were already weather services you can use, but seeing Apple integrate it as a very usable service, much like MapKit, is very cool. I can see lots of useful ways to combine it with location-aware apps. \n- **Generics and Existential's**: Ahh, I have really been looking forward to the generics and existential upgrades we're getting. `any Thing` and `some Collection<Thing>` really spoke to me. If you haven't dealt a lot with generics you might be thinking \"So what?\" But for those who have, this exposes lots of really fun new possibilities. \n- **Regex Builders**: So I love regex, and thus I am somewhat excited for regex literals...but builders are frankly way more sexy. Regex is not *remotely* approachable for people. It's weird to learn, it's terse and archaic. That said, builders are very approachable and I think we're going to see people build some really powerful parsers using regex builders. MAYBE we will even see people encouraged to stay away from regex when it's not the right tool for the job.\n- **Swift Package Plugins**: Look! We almost have a for reals build chain! It's so exciting. I love that this is driven through SPM and I think we'll see some really impressive work done with code generation and Xcode integration now that this is available.\n- **Distributed Actors**: I've had my eye on you, distributed actors. So here's the thing, this won't speak to everybody. But if you're a fan of Swift on server (I am!) then you will see potential here. I have also developed a few different enterprise apps where we needed a way to communicate cross-process. Distributed actors could be just the thing!\n- **DocC Updates**: Okay, they didn't mention this today, but trust me, it's a thing. DocC supports static site generation, in fact, I just got this working with one of my open source projects recently. I know documentation sucks for developers, but I genuinely think some of the upcoming changes are going to make it suck a lot less.\n\n### The Blasè\n- **Xcode Cloud**: This took forever to release. I have been really eager to see it but was really disheartened by the pricing I saw today. I do a lot of open source work, I wanted to see a \"free to use for OSS\" statement, but didn't get it. 20 hours of build time free is nothing to sneeze at, but then the pricing gets unrealistic for individual developers. Why would you choose this when there are very good CI/CD solutions you can use for free?\n- **Lock Screen Updates, Map Updates, General UI things**: Don't get me wrong, I had several \"ooh, that looks good\" moments when I saw this. Particularly when the map went into dark mode and buildings lit up. However, these updates aren't sexy, they're delightful. As a developer, they are just sort of amusing.\n- **New Hardware and M2**: Look this is cool, but I use a desktop. I'll be investing in a Mac Studio soon. It's great you managed to shave off some pounds and make the air EVEN SMALLER but that's not what speaks to me. I'll think this is more exciting when they're announce the M2 Max Ultra Super Duper (their naming convention is getting weird). For portability I use an iPad, it does great.\n- **Apple Pay Later**: Once again, I think this is cool as a consumer. As a developer it's not all that exciting because you integrate the exact same way. \n- **Metal Updates**: I think game devs are very impressive, but the thing is Metal isn't approachable for beginners and AAA game companies aren't gonna be interested before C++ interop is a thing. They're headed in the right direction, but I don't predict we'll suddenly see major releases targeting macOS as a priority.\n- **The Rest**: Yeah, I know there's a lot more I didn't mention. That's because it didn't make a strong impression on me as an engineer. I think there's some cool things if you're a consumer of Apple products, my wife will be really happy about some of the new iPad things, but nothing else changes the way I code or much of my personal workflow.\n\n### The \"It's About Time\"\n- **iMessage Editing**: Seriously Apple?! It took you to 2022 to do this? I mean I'm glad it exists but I can't help but wonder if the insistence on your own protocol and own messaging system caused these issues. You could've used a standard and had this available much earlier.\n- **SwiftUI NavigationView Updates**: Okay, I added this in \"The Sexy\" category, because I've been wanting it for a long time. That said, navigation has been laughably bad for complex workflows in SwiftUI since day one, so \"it's about time\".\n\n### The Missing\n- **RealityOS**: We all know it's coming, frankly I'm glad they're taking their time. I, and many others, believe that Apple glasses will eventually be announced. I want them to take their time and get it right, we don't need a repeat of Google Glass.\n- **Other SwiftUI Navigation Patterns**: So...nav stacks *might* be more sane now, but what about modals? What about other forms of presentation? I want control over the stack!\n- **Variadic Generics**: There's been multiple proposals here, but I really hope this is coming soon. This would solve a lot of weirdness for result builders and arity issues. To put that in english, your SwiftUI views could have more than 10 things without having to use a group.\n- **Reflection**: Am I the only one who thinks Swift needs for reals reflection? Better reflection would allow us new dependency injection frameworks and actual mocking frameworks. It's a little baffling to me this is never on the radar.\n- **Xcode for iPad**: WHY APPLE?! Why won't you just do it? The iPad is running on M1, this seems feasible. Or at least make Swift Playgrounds a better dev environment. I was really hoping this'd be announced this year.",
                                 author: tyler)
        try await wwdc2022.save(on: database)
        try await wwdc2022.$tags.attach([swift, WWDC], on: database)
    }
    
    func revert(on database: any Database) async throws {
        try await Tag.find(UUID(uuidString: "75642bd6-f139-4c4a-b77e-3e93a5dcaab4"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "9a2e3819-bade-418a-abde-15f947452b67"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "4926c474-487f-4b6f-8c9f-862026a5d005"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "a79c8461-63ed-4663-a11f-b98b02a55e41"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "83139341-4348-4ff2-bfc3-8fae7e0d271b"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "b462869f-b46e-4ed6-8a47-1dd1b9c289be"), on: database)?.delete(on: database)
        try await Tag.find(UUID(uuidString: "675afeab-70c5-4e00-b39c-5686b3ed780d"), on: database)?.delete(on: database)

        try await BlogPost.find(UUID(uuidString: "b35c209e-d2c1-475b-86af-333294c9407e"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "1665046f-18ba-4005-9122-b708a28640dc"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "de43ae83-2feb-4b2f-9a7d-25f6078fb547"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "fa44f008-9f2e-4152-af22-c9359d1d206c"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "4d4b70cc-1bbd-4505-a40e-33376d494302"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "fde72d71-6b59-49a6-bca5-419c12835348"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "733037c5-960e-4ed8-8190-d1fd72fb24d6"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "bc561082-a34b-4a53-8877-dec62d28ffd8"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "7bc345f7-1587-4fdd-885c-ad254890f67e"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "b51b5885-d3f5-4082-b634-383cd90443ab"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "798d115c-4268-4236-84e9-d290f47cf450"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "5675d0e5-f69d-4b89-a642-6d6ac979ce83"), on: database)?.delete(on: database)
        try await BlogPost.find(UUID(uuidString: "274feaf9-5219-4d3d-9c09-6e8620e52db2"), on: database)?.delete(on: database)
    }
}
